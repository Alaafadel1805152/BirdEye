{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3b67ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import hypot\n",
    "from PIL import Image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import time\n",
    "import mediapipe as mp\n",
    "import dlib\n",
    "import pyautogui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ae531a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midpoint(p1 ,p2):\n",
    "    return int((p1.x + p2.x)/2), int((p1.y + p2.y)/2)\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "whT = 320\n",
    "confThreshold = 0.5\n",
    "nmsThreshold = 0.2\n",
    "classesFile = \"coco.names\"\n",
    "classNames =[]\n",
    "with open(classesFile, 'rt') as f:\n",
    "    classNames = f.read().splitlines()\n",
    "\n",
    "## Model Files\n",
    "modelConfiguration = \"yolov3-320.cfg\"\n",
    "modelWeights = \"yolov3.weights\"\n",
    "net = cv2.dnn.readNetFromDarknet(modelConfiguration, modelWeights)   #create network\n",
    "net.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n",
    "net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c472b8",
   "metadata": {},
   "source": [
    "# Face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38709df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:15: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\RAHEEM2000\\AppData\\Local\\Temp\\ipykernel_536\\4171245566.py:15: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "detector=dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "model = load_model('facefeatures_new_model.h5')\n",
    "\n",
    "# Loading the cascades\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6808dc4c",
   "metadata": {},
   "source": [
    "# Eye tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe3dd72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaze_ratio(eye_points, facial_landmarks):\n",
    "    left_eye_region = np.array([(facial_landmarks.part(eye_points[0]).x, facial_landmarks.part(eye_points[0]).y),\n",
    "                                (facial_landmarks.part(eye_points[1]).x, facial_landmarks.part(eye_points[1]).y),\n",
    "                                (facial_landmarks.part(eye_points[2]).x, facial_landmarks.part(eye_points[2]).y),\n",
    "                                (facial_landmarks.part(eye_points[3]).x, facial_landmarks.part(eye_points[3]).y),\n",
    "                                (facial_landmarks.part(eye_points[4]).x, facial_landmarks.part(eye_points[4]).y),\n",
    "                                (facial_landmarks.part(eye_points[5]).x, facial_landmarks.part(eye_points[5]).y)], np.int32)\n",
    "    # cv2.polylines(frame, [left_eye_region], True, (0, 0, 255), 2)\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    mask = np.zeros((height, width), np.uint8)\n",
    "    cv2.polylines(mask, [left_eye_region], True, 255, 2)\n",
    "    cv2.fillPoly(mask, [left_eye_region], 255)\n",
    "    eye = cv2.bitwise_and(gray, gray, mask=mask)\n",
    "\n",
    "    min_x = np.min(left_eye_region[:, 0])\n",
    "    max_x = np.max(left_eye_region[:, 0])\n",
    "    min_y = np.min(left_eye_region[:, 1])\n",
    "    max_y = np.max(left_eye_region[:, 1])\n",
    "\n",
    "    gray_eye = eye[min_y: max_y, min_x: max_x]\n",
    "    _, threshold_eye = cv2.threshold(gray_eye, 70, 255, cv2.THRESH_BINARY)\n",
    "    height, width = threshold_eye.shape\n",
    "    left_side_threshold = threshold_eye[0: height, 0: int(width / 2)]\n",
    "    left_side_white = cv2.countNonZero(left_side_threshold)\n",
    "\n",
    "    right_side_threshold = threshold_eye[0: height, int(width / 2): width]\n",
    "    right_side_white = cv2.countNonZero(right_side_threshold)\n",
    "\n",
    "    if left_side_white == 0:\n",
    "        gaze_ratio = 1\n",
    "    elif right_side_white == 0:\n",
    "        gaze_ratio = 5\n",
    "    else:\n",
    "        gaze_ratio = left_side_white / right_side_white\n",
    "    return gaze_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5fc63",
   "metadata": {},
   "source": [
    "# Face gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c7b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "class mpFaceMesh:\n",
    "    import mediapipe as mp\n",
    "    def __init__(self,still=False,numFaces=3,tol1=.5,tol2=.5,drawMesh=True):\n",
    "        self.myFaceMesh=self.mp.solutions.face_mesh.FaceMesh()\n",
    "        self.myDraw=self.mp.solutions.drawing_utils\n",
    "        self.draw=drawMesh\n",
    "    def Marks(self,frame):\n",
    "        global width\n",
    "        global height\n",
    "        drawSpecCircle=self.myDraw.DrawingSpec(thickness=0,circle_radius=0,color=(0,0,255))\n",
    "        drawSpecLine=self.myDraw.DrawingSpec(thickness=1,circle_radius=2,color=(255,0,0))\n",
    "        frameRGB=cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)\n",
    "        results=self.myFaceMesh.process(frameRGB)\n",
    "        facesMeshLandmarks=[]\n",
    "        left_length=0\n",
    "        right_length = 0\n",
    "        upLeft_length = 0\n",
    "        upRight_length = 0\n",
    "\n",
    "        if results.multi_face_landmarks !=None:\n",
    "            for faceMesh in results.multi_face_landmarks:\n",
    "                faceMeshLandmarks=[]\n",
    "                for lm in faceMesh.landmark:\n",
    "                    loc=(int(lm.x*width),int(lm.y*height))\n",
    "                    faceMeshLandmarks.append(loc)\n",
    "                facesMeshLandmarks.append(faceMeshLandmarks)\n",
    "\n",
    "                left_length=math.sqrt( (faceMeshLandmarks[0][0]-faceMeshLandmarks[49][0])**2+(faceMeshLandmarks[0][0]-faceMeshLandmarks[49][1])**2)   # left\n",
    "\n",
    "                right_length= math.sqrt( (faceMeshLandmarks[0][0]-faceMeshLandmarks[279][0])**2+(faceMeshLandmarks[0][0]-faceMeshLandmarks[279][1])**2) # right\n",
    "\n",
    "                upLeft_length =math.sqrt((faceMeshLandmarks[0][0] - faceMeshLandmarks[65][0]) ** 2 + (faceMeshLandmarks[0][0] - faceMeshLandmarks[65][1]) ** 2)  # up left\n",
    "\n",
    "                upRight_length=math.sqrt((faceMeshLandmarks[0][0] - faceMeshLandmarks[295][0]) ** 2 + (faceMeshLandmarks[0][0] - faceMeshLandmarks[295][1]) ** 2) # up right\n",
    "\n",
    "\n",
    "                if left_length<200:\n",
    "                    print(\"looking right\")\n",
    "                if left_length>400:\n",
    "                    print(\"looking left\")\n",
    "                # if upLeft_length<200:\n",
    "                #     print(\"looking up left\")\n",
    "                # if upRight_length<200:\n",
    "                #     print(\"looking up right\")\n",
    "\n",
    "\n",
    "\n",
    "                if self.draw==True:\n",
    "                    self.myDraw.draw_landmarks(frame,faceMesh,self.mp.solutions.face_mesh.FACEMESH_TESSELATION,drawSpecCircle,drawSpecLine)\n",
    "        return facesMeshLandmarks\n",
    "\n",
    "class mpFace:   ###read the face and get the topLeft/bottomRight of detection box\n",
    "    import mediapipe as mp\n",
    "    def __init__(self):\n",
    "        self.myFace=self.mp.solutions.face_detection.FaceDetection()\n",
    "    def Marks(self,frame):\n",
    "        faceBoundBoxs=[]\n",
    "        if results.detections != None:\n",
    "            for face in results.detections:\n",
    "                bBox=face.location_data.relative_bounding_box\n",
    "                topLeft=(int(bBox.xmin*width),int(bBox.ymin*height))\n",
    "                bottomRight=(int((bBox.xmin+bBox.width)*width),int((bBox.ymin+bBox.height)*height))\n",
    "                faceBoundBoxs.append((topLeft,bottomRight))\n",
    "\n",
    "        return faceBoundBoxs\n",
    "\n",
    "class mpPose:\n",
    "    import mediapipe as mp\n",
    "    def __init__(self,still=False,upperBody=False, smoothData=True, tol1=.5, tol2=.5):\n",
    "        self.myPose=self.mp.solutions.pose.Pose(still,upperBody,smoothData,tol1,tol2)\n",
    "    def Marks(self,frame):     \n",
    "        poseLandmarks=[]\n",
    "        if results.pose_landmarks:\n",
    "            for lm in results.pose_landmarks.landmark:\n",
    "                poseLandmarks.append((int(lm.x*width),int(lm.y*height)))\n",
    "        return poseLandmarks\n",
    "\n",
    "\n",
    "width=1280\n",
    "height=720\n",
    "\n",
    "\n",
    "\n",
    "findFace=mpFace()\n",
    "# findPose=mpPose()\n",
    "findMesh=mpFaceMesh(drawMesh=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0aad46c",
   "metadata": {},
   "source": [
    "# object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c0dd085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findObjects(outputs, img):\n",
    "    hT, wT, cT = img.shape\n",
    "    bbox =[]         ##contain x-y depth and height\n",
    "    classIds =[]\n",
    "    confs = []\n",
    "    for output in outputs:\n",
    "        for det in output:\n",
    "            scores = det [5:]\n",
    "            classId = np.argmax(scores)\n",
    "            confidence = scores[classId]\n",
    "            # print(classId)\n",
    "            # print(confidence)\n",
    "            if confidence > confThreshold:\n",
    "                w, h = int(det[2]*wT) , int(det[3]*hT)\n",
    "                x, y=int((det [0]*wT)-w/2) , int((det[1]*hT)-h/2)\n",
    "                bbox.append( [x,y,w,h])\n",
    "                if classId==67:\n",
    "                    print('alert!!!')\n",
    "                classIds.append(classId)\n",
    "                confs.append(float(confidence))\n",
    "\n",
    "\n",
    "\n",
    "    indices = cv2.dnn.NMSBoxes(bbox, confs, confThreshold, nmsThreshold)\n",
    "    for i in indices:\n",
    "        box = bbox[i]\n",
    "        x,y,w,h= box [0],box[1],box[2],box[3]\n",
    "        # print(x,y,w,h)\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 255), 2)\n",
    "        cv2.putText(img, f'{classNames [classIds[i]].upper()} {int(confs[i]*100)}%',\n",
    "        (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 255), 2)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617cf814",
   "metadata": {},
   "source": [
    "# Screen recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be351d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify resolution\n",
    "resolution = (1920, 1080)\n",
    "  \n",
    "# Specify video codec\n",
    "codec = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "  \n",
    "# Specify name of Output file\n",
    "filename = \"Recording.avi\"\n",
    "  \n",
    "# Specify frames rate. We can choose any \n",
    "# value and experiment with it\n",
    "fps = 10.0\n",
    "  \n",
    "  \n",
    "# Creating a VideoWriter object\n",
    "out = cv2.VideoWriter(filename, codec, fps, resolution)\n",
    "  \n",
    "# Create an Empty window\n",
    "cv2.namedWindow(\"Live\", cv2.WINDOW_NORMAL)\n",
    "  \n",
    "# Resize this window\n",
    "cv2.resizeWindow(\"Live\", 480, 270)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68503e04",
   "metadata": {},
   "source": [
    "# Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02521a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "face=multiprocessing.Process(target=face_extractor)\n",
    "eye=multiprocessing.Process(target=get_gaze_ratio)\n",
    "face.start()\n",
    "eye.start()\n",
    "\n",
    "face.join()\n",
    "eye.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7074eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "alert!!!\n",
      "looking right\n",
      "alert!!!\n",
      "alert!!!\n",
      "alert!!!\n",
      "looking right\n",
      "looking right\n",
      "alert!!!\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "alert!!!\n",
      "looking right\n",
      "alert!!!\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "alert!!!\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "alert!!!\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n"
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    _, frame = video_capture.read()\n",
    "    _, frame2 = video_capture.read()\n",
    "    #canvas = detect(gray, frame)\n",
    "    #image, face =face_detector(frame)\n",
    "    new_frame = np.zeros((500, 500, 3), np.uint8)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    facesMeshLM=findMesh.Marks(frame2)\n",
    "    faces = detector(gray)\n",
    "    face=face_extractor(frame)\n",
    "    if type(face) is np.ndarray:\n",
    "        face = cv2.resize(face, (224, 224))\n",
    "        im = Image.fromarray(face, 'RGB')\n",
    "           #Resizing into 128x128 because we trained the model with this image size.\n",
    "        img_array = np.array(im)\n",
    "                    #Our keras model used a 4D tensor, (images x height x width x channel)\n",
    "                    #So changing dimension 128x128x3 into 1x128x128x3 \n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "        \n",
    "    else: \n",
    "     cv2.putText(frame,\"No Face Found\",(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),2)\n",
    "        \n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        # Gaze detection\n",
    "        gaze_ratio_left_eye = get_gaze_ratio([36, 37, 38, 39, 40, 41], landmarks)\n",
    "        gaze_ratio_right_eye = get_gaze_ratio([42, 43, 44, 45, 46, 47], landmarks)\n",
    "        gaze_ratio = (gaze_ratio_right_eye + gaze_ratio_left_eye) / 2\n",
    "        if gaze_ratio <= 1:\n",
    "            cv2.putText(frame, \"RIGHT\", (50, 100), font, 2, (0, 0, 255), 3)\n",
    "            new_frame[:] = (0, 0, 255)\n",
    "        elif 1 < gaze_ratio < 2.11:\n",
    "            cv2.putText(frame, \"Left\", (50, 100), font, 2, (0, 0, 255), 3)\n",
    "        else:\n",
    "            new_frame[:] = (255, 0, 0)\n",
    "            cv2.putText(frame, \"CENTER\", (50, 100), font, 2, (0, 0, 255), 3)\n",
    "\n",
    "    success, img = video_capture.read()\n",
    "    blob = cv2.dnn.blobFromImage(img, 1 / 255, (whT, whT), [0,0,0],1,crop=False)      ###network accept input in type blob, so change image to blob here ##whT, whT=>width and wight and target\n",
    "    net.setInput(blob)\n",
    "    layersNames = net.getLayerNames()  ###names of all our layers(extraction layer , out put layers)\n",
    "    outputNames=[]\n",
    "\n",
    "    outputNames = [(layersNames[i - 1]) for i in net.getUnconnectedOutLayers()]\n",
    "    outputs = net.forward(outputNames)\n",
    "    findObjects(outputs, img)\n",
    "    img = pyautogui.screenshot()\n",
    "  \n",
    "    # Convert the screenshot to a numpy array\n",
    "    frame = np.array(img)\n",
    "  \n",
    "    # Convert it from BGR(Blue, Green, Red) to\n",
    "    # RGB(Red, Green, Blue)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "  \n",
    "    # Write it to the output file\n",
    "    out.write(frame)\n",
    "      \n",
    "    # Optional: Display the recording screen\n",
    "    cv2.imshow('Live', frame)\n",
    "       \n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "        \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "        \n",
    "       \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3d3b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ad4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
