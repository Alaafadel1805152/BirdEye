{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4f3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "from math import hypot\n",
    "from PIL import Image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import json\n",
    "import random\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21c1f30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def midpoint(p1 ,p2):\n",
    "    return int((p1.x + p2.x)/2), int((p1.y + p2.y)/2)\n",
    "font = cv2.FONT_HERSHEY_PLAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f3864c",
   "metadata": {},
   "source": [
    "# Face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5863cde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:15: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:15: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\RAHEEM~1\\AppData\\Local\\Temp/ipykernel_8552/4171245566.py:15: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "detector=dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "model = load_model('facefeatures_new_model.h5')\n",
    "\n",
    "# Loading the cascades\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_extractor(img):\n",
    "    # Function detects faces and returns the cropped face\n",
    "    # If no face detected, it returns the input image\n",
    "    \n",
    "    #gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    # Crop all faces found\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4d5581",
   "metadata": {},
   "source": [
    "# Eye tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3c68025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaze_ratio(eye_points, facial_landmarks):\n",
    "    left_eye_region = np.array([(facial_landmarks.part(eye_points[0]).x, facial_landmarks.part(eye_points[0]).y),\n",
    "                                (facial_landmarks.part(eye_points[1]).x, facial_landmarks.part(eye_points[1]).y),\n",
    "                                (facial_landmarks.part(eye_points[2]).x, facial_landmarks.part(eye_points[2]).y),\n",
    "                                (facial_landmarks.part(eye_points[3]).x, facial_landmarks.part(eye_points[3]).y),\n",
    "                                (facial_landmarks.part(eye_points[4]).x, facial_landmarks.part(eye_points[4]).y),\n",
    "                                (facial_landmarks.part(eye_points[5]).x, facial_landmarks.part(eye_points[5]).y)], np.int32)\n",
    "    # cv2.polylines(frame, [left_eye_region], True, (0, 0, 255), 2)\n",
    "\n",
    "    height, width, _ = frame.shape\n",
    "    mask = np.zeros((height, width), np.uint8)\n",
    "    cv2.polylines(mask, [left_eye_region], True, 255, 2)\n",
    "    cv2.fillPoly(mask, [left_eye_region], 255)\n",
    "    eye = cv2.bitwise_and(gray, gray, mask=mask)\n",
    "\n",
    "    min_x = np.min(left_eye_region[:, 0])\n",
    "    max_x = np.max(left_eye_region[:, 0])\n",
    "    min_y = np.min(left_eye_region[:, 1])\n",
    "    max_y = np.max(left_eye_region[:, 1])\n",
    "\n",
    "    gray_eye = eye[min_y: max_y, min_x: max_x]\n",
    "    _, threshold_eye = cv2.threshold(gray_eye, 70, 255, cv2.THRESH_BINARY)\n",
    "    height, width = threshold_eye.shape\n",
    "    left_side_threshold = threshold_eye[0: height, 0: int(width / 2)]\n",
    "    left_side_white = cv2.countNonZero(left_side_threshold)\n",
    "\n",
    "    right_side_threshold = threshold_eye[0: height, int(width / 2): width]\n",
    "    right_side_white = cv2.countNonZero(right_side_threshold)\n",
    "\n",
    "    if left_side_white == 0:\n",
    "        gaze_ratio = 1\n",
    "    elif right_side_white == 0:\n",
    "        gaze_ratio = 5\n",
    "    else:\n",
    "        gaze_ratio = left_side_white / right_side_white\n",
    "    return gaze_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1797f3e4",
   "metadata": {},
   "source": [
    "# Face gestures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db9e5ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "class mpFaceMesh:\n",
    "    import mediapipe as mp\n",
    "    def __init__(self,still=False,numFaces=3,tol1=.5,tol2=.5,drawMesh=True):\n",
    "        self.myFaceMesh=self.mp.solutions.face_mesh.FaceMesh()\n",
    "        self.myDraw=self.mp.solutions.drawing_utils\n",
    "        self.draw=drawMesh\n",
    "    def Marks(self,frame):\n",
    "        global width\n",
    "        global height\n",
    "        drawSpecCircle=self.myDraw.DrawingSpec(thickness=0,circle_radius=0,color=(0,0,255))\n",
    "        drawSpecLine=self.myDraw.DrawingSpec(thickness=1,circle_radius=2,color=(255,0,0))\n",
    "        frameRGB=cv2.cvtColor(frame2,cv2.COLOR_BGR2RGB)\n",
    "        results=self.myFaceMesh.process(frameRGB)\n",
    "        facesMeshLandmarks=[]\n",
    "        left_length=0\n",
    "        right_length = 0\n",
    "        upLeft_length = 0\n",
    "        upRight_length = 0\n",
    "\n",
    "        if results.multi_face_landmarks !=None:\n",
    "            for faceMesh in results.multi_face_landmarks:\n",
    "                faceMeshLandmarks=[]\n",
    "                for lm in faceMesh.landmark:\n",
    "                    loc=(int(lm.x*width),int(lm.y*height))\n",
    "                    faceMeshLandmarks.append(loc)\n",
    "                facesMeshLandmarks.append(faceMeshLandmarks)\n",
    "\n",
    "                left_length=math.sqrt( (faceMeshLandmarks[0][0]-faceMeshLandmarks[49][0])**2+(faceMeshLandmarks[0][0]-faceMeshLandmarks[49][1])**2)   # left\n",
    "\n",
    "                right_length= math.sqrt( (faceMeshLandmarks[0][0]-faceMeshLandmarks[279][0])**2+(faceMeshLandmarks[0][0]-faceMeshLandmarks[279][1])**2) # right\n",
    "\n",
    "                upLeft_length =math.sqrt((faceMeshLandmarks[0][0] - faceMeshLandmarks[65][0]) ** 2 + (faceMeshLandmarks[0][0] - faceMeshLandmarks[65][1]) ** 2)  # up left\n",
    "\n",
    "                upRight_length=math.sqrt((faceMeshLandmarks[0][0] - faceMeshLandmarks[295][0]) ** 2 + (faceMeshLandmarks[0][0] - faceMeshLandmarks[295][1]) ** 2) # up right\n",
    "\n",
    "\n",
    "                if left_length<200:\n",
    "                    print(\"looking right\")\n",
    "                if left_length>400:\n",
    "                    print(\"looking left\")\n",
    "                # if upLeft_length<200:\n",
    "                #     print(\"looking up left\")\n",
    "                # if upRight_length<200:\n",
    "                #     print(\"looking up right\")\n",
    "\n",
    "\n",
    "\n",
    "                if self.draw==True:\n",
    "                    self.myDraw.draw_landmarks(frame,faceMesh,self.mp.solutions.face_mesh.FACEMESH_TESSELATION,drawSpecCircle,drawSpecLine)\n",
    "        return facesMeshLandmarks\n",
    "\n",
    "class mpFace:   ###read the face and get the topLeft/bottomRight of detection box\n",
    "    import mediapipe as mp\n",
    "    def __init__(self):\n",
    "        self.myFace=self.mp.solutions.face_detection.FaceDetection()\n",
    "    def Marks(self,frame):\n",
    "        faceBoundBoxs=[]\n",
    "        if results.detections != None:\n",
    "            for face in results.detections:\n",
    "                bBox=face.location_data.relative_bounding_box\n",
    "                topLeft=(int(bBox.xmin*width),int(bBox.ymin*height))\n",
    "                bottomRight=(int((bBox.xmin+bBox.width)*width),int((bBox.ymin+bBox.height)*height))\n",
    "                faceBoundBoxs.append((topLeft,bottomRight))\n",
    "\n",
    "        return faceBoundBoxs\n",
    "\n",
    "class mpPose:\n",
    "    import mediapipe as mp\n",
    "    def __init__(self,still=False,upperBody=False, smoothData=True, tol1=.5, tol2=.5):\n",
    "        self.myPose=self.mp.solutions.pose.Pose(still,upperBody,smoothData,tol1,tol2)\n",
    "    def Marks(self,frame):     \n",
    "        poseLandmarks=[]\n",
    "        if results.pose_landmarks:\n",
    "            for lm in results.pose_landmarks.landmark:\n",
    "                poseLandmarks.append((int(lm.x*width),int(lm.y*height)))\n",
    "        return poseLandmarks\n",
    "\n",
    "\n",
    "width=1280\n",
    "height=720\n",
    "\n",
    "\n",
    "\n",
    "findFace=mpFace()\n",
    "# findPose=mpPose()\n",
    "findMesh=mpFaceMesh(drawMesh=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f1502",
   "metadata": {},
   "source": [
    "# Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2baf8853",
   "metadata": {},
   "outputs": [],
   "source": [
    "face=multiprocessing.Process(target=face_extractor)\n",
    "eye=multiprocessing.Process(target=get_gaze_ratio)\n",
    "face.start()\n",
    "eye.start()\n",
    "\n",
    "face.join()\n",
    "eye.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78700dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking left\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking left\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n",
      "looking right\n"
     ]
    }
   ],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    _, frame = video_capture.read()\n",
    "    _, frame2 = video_capture.read()\n",
    "    #canvas = detect(gray, frame)\n",
    "    #image, face =face_detector(frame)\n",
    "    new_frame = np.zeros((500, 500, 3), np.uint8)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    facesMeshLM=findMesh.Marks(frame2)\n",
    "    faces = detector(gray)\n",
    "    face=face_extractor(frame)\n",
    "    if type(face) is np.ndarray:\n",
    "        face = cv2.resize(face, (224, 224))\n",
    "        im = Image.fromarray(face, 'RGB')\n",
    "           #Resizing into 128x128 because we trained the model with this image size.\n",
    "        img_array = np.array(im)\n",
    "                    #Our keras model used a 4D tensor, (images x height x width x channel)\n",
    "                    #So changing dimension 128x128x3 into 1x128x128x3 \n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "    \n",
    "        \n",
    "    else: \n",
    "     cv2.putText(frame,\"No Face Found\",(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),2)\n",
    "        \n",
    "    for face in faces:\n",
    "        landmarks = predictor(gray, face)\n",
    "        # Gaze detection\n",
    "        gaze_ratio_left_eye = get_gaze_ratio([36, 37, 38, 39, 40, 41], landmarks)\n",
    "        gaze_ratio_right_eye = get_gaze_ratio([42, 43, 44, 45, 46, 47], landmarks)\n",
    "        gaze_ratio = (gaze_ratio_right_eye + gaze_ratio_left_eye) / 2\n",
    "        if gaze_ratio <= 1:\n",
    "            cv2.putText(frame, \"RIGHT\", (50, 100), font, 2, (0, 0, 255), 3)\n",
    "            new_frame[:] = (0, 0, 255)\n",
    "        elif 1 < gaze_ratio < 2.11:\n",
    "            cv2.putText(frame, \"Left\", (50, 100), font, 2, (0, 0, 255), 3)\n",
    "        else:\n",
    "            new_frame[:] = (255, 0, 0)\n",
    "            cv2.putText(frame, \"CENTER\", (50, 100), font, 2, (0, 0, 255), 3)\n",
    "\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "        \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:\n",
    "        break\n",
    "        \n",
    "       \n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f896118",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
